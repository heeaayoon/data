# Law_RAG_Run.py

import pandas as pd
import numpy as np
import os
from tqdm import tqdm
import json
import traceback
from pathlib import Path
from sentence_transformers import SentenceTransformer, CrossEncoder
import ollama

# utils ëª¨ë“ˆì—ì„œ í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from Law_RAG_Utils import (
    load_laws_from_csv,
    retrieve_top_k_laws,
    rerank_with_cross_encoder,
    expand_and_extract_keywords,
    generate_draft_answer,
    evaluate_and_refine_answer,
    format_final_report_from_json
)

# ì„¤ì • 
DATA_DIR = "data"
PROCESSED_DIR = "processed"
LAW_FILENAME = "law_total.csv"
NEWS_FILENAME = "news_cluster_w_event_name.csv"
NEWS_CONTENT_COLUMN = "representative"
OUTPUT_FILENAME = "Law_RAG_Result.csv"

LLM_MODEL_NAME = 'command-r'
RETRIEVAL_MODEL_NAME = 'jhgan/ko-sroberta-multitask'
RERANKER_MODEL_NAME = 'bongsoo/klue-cross-encoder-v1'

# NUM_ARTICLES_TO_TEST = 10 # í…ŒìŠ¤íŠ¸
NUM_ARTICLES_TO_TEST = None # ì „ì²´ ì‹¤í–‰ ì‹œì—ëŠ” None ìœ¼ë¡œ ì„¤ì •

def main():
    """RAG íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì‹¤í–‰í•˜ê³ , ì›ë³¸ ë°ì´í„°ì— 2ê°œ ì—´ë§Œ ì¶”ê°€í•©ë‹ˆë‹¤."""
    
    # 1. ê²½ë¡œ ì¤€ë¹„
    base_path = Path(__file__).resolve().parent.parent
    processed_data_dir = base_path / DATA_DIR / PROCESSED_DIR
    law_path = processed_data_dir / LAW_FILENAME
    news_path = processed_data_dir / NEWS_FILENAME
    output_path = processed_data_dir / OUTPUT_FILENAME
    embedding_cache_path = processed_data_dir / 'law_embeddings_1500char.npy'

    # 2. Ollama ì„œë²„ ì—°ê²° í™•ì¸
    print("\n--- Ollama ì„œë²„ ì—°ê²° í™•ì¸ ì¤‘ ---")
    try:
        ollama.list()
        print("Ollama ì„œë²„ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print("\n[ì¤‘ìš”] Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("   - Ollama ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì‹¤í–‰ ì¤‘ì¸ì§€, ë˜ëŠ” `ollama serve` ëª…ë ¹ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."); return

    # 3. ë°ì´í„° ë° ëª¨ë¸ ë¡œë”©
    law_chunks = load_laws_from_csv(law_path)
    if not law_chunks: return
    unique_law_names = sorted(list(set(chunk['law_name'] for chunk in law_chunks)))
    print(f"\nì´ {len(law_chunks)}ê°œ ë²•ë¥  ì¡°í•­ ë¡œë“œ ì™„ë£Œ. ({len(unique_law_names)}ê°œ ë²•ë¥ )")
    
    print("\nëª¨ë¸ ë¡œë“œ ì¤‘...")
    retrieval_model = SentenceTransformer(RETRIEVAL_MODEL_NAME)
    cross_encoder = CrossEncoder(RERANKER_MODEL_NAME)

    # 4. ì„ë² ë”© ìƒì„± ë˜ëŠ” ë¡œë”©
    corpus_embeddings_cpu = None
    if embedding_cache_path.exists():
        print(f"ì €ì¥ëœ ë²•ë¥  ì„ë² ë”© '{embedding_cache_path.name}' ë¡œë“œ ì¤‘...")
        loaded_embeddings = np.load(embedding_cache_path)
        if len(loaded_embeddings) == len(law_chunks):
            corpus_embeddings_cpu = loaded_embeddings
        else: print("ì„ë² ë”©ê³¼ ë²•ë¥  ë°ì´í„° ê°œìˆ˜ê°€ ë¶ˆì¼ì¹˜í•˜ì—¬ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    
    if corpus_embeddings_cpu is None:
        print(f"{len(law_chunks)}ê°œ ë²•ë¥  ì¡°í•­ ì„ë² ë”© ìƒì„± ì¤‘...")
        corpus_texts = [chunk['text_for_embedding'] for chunk in law_chunks]
        corpus_embeddings = retrieval_model.encode(corpus_texts, convert_to_tensor=True, show_progress_bar=True)
        corpus_embeddings_cpu = corpus_embeddings.cpu().numpy()
        np.save(embedding_cache_path, corpus_embeddings_cpu)
        print(f"ìƒˆë¡œìš´ ì„ë² ë”©ì„ '{embedding_cache_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    # 5. ë‰´ìŠ¤ ê¸°ì‚¬ ë¡œë”© ë° ë¶„ì„ ì‹œì‘
    try:
        df_news_original = pd.read_csv(news_path, encoding='utf-8-sig')
        if NEWS_CONTENT_COLUMN not in df_news_original.columns:
            print(f"ì˜¤ë¥˜: ë‰´ìŠ¤ íŒŒì¼ì— '{NEWS_CONTENT_COLUMN}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."); return
        
        df_to_analyze = df_news_original.dropna(subset=[NEWS_CONTENT_COLUMN]).drop_duplicates(subset=[NEWS_CONTENT_COLUMN]).copy()
        print(f"'{news_path.name}'ì—ì„œ ì¤‘ë³µ ì œì™¸ {len(df_to_analyze)}ê°œ ëŒ€í‘œ ê¸°ì‚¬ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ.")
    except FileNotFoundError:
        print(f"ë‰´ìŠ¤ íŒŒì¼ '{news_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return
    
    analysis_results = []
    temp_output_path = base_path / f"{output_path.stem}_temp.csv"
    num_to_run = len(df_to_analyze) if NUM_ARTICLES_TO_TEST is None else min(NUM_ARTICLES_TO_TEST, len(df_to_analyze))
    
    print(f"\n--- ì´ {num_to_run}ê°œ ê¸°ì‚¬ì— ëŒ€í•´ ë¶„ì„ ì‹œì‘ ---")
    
    for index, row in tqdm(df_to_analyze.head(num_to_run).iterrows(), total=num_to_run, desc="ê¸°ì‚¬ ë¶„ì„ ì¤‘"):
        content = row[NEWS_CONTENT_COLUMN]
        print(f"\n\n{'='*30} ê¸°ì‚¬ {index+1} ë¶„ì„ ì‹œì‘ {'='*30}")
        try:
            virtual_law_clause, keywords = expand_and_extract_keywords(content, LLM_MODEL_NAME)
            
            queries_for_retrieval = [content, virtual_law_clause, keywords]
            aggregated_candidates = {}
            for q in queries_for_retrieval:
                if not q: continue
                results = retrieve_top_k_laws(q, law_chunks, retrieval_model, corpus_embeddings_cpu, top_k=50)
                for cand, score in results:
                    unique_key = f"[{cand['law_name']}] {cand['id']}"
                    if unique_key not in aggregated_candidates or score > aggregated_candidates[unique_key][1]:
                        aggregated_candidates[unique_key] = (cand, score)
            
            sorted_initial_results = sorted(aggregated_candidates.values(), key=lambda item: item[1], reverse=True)
            initial_candidates = [item[0] for item in sorted_initial_results]
            
            reranker_query = f"{virtual_law_clause} {keywords}".strip() or content
            final_candidates = rerank_with_cross_encoder(reranker_query, initial_candidates, cross_encoder, top_n=20)
            
            top_5_laws = final_candidates[:5]
            final_analysis_json = None
            last_suggestion = "ìµœì´ˆ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤."
            for iter_num in range(2):
                print(f"\n--- ğŸ” ë¶„ì„ ë° ê°œì„  ì‹œë„ ({iter_num + 1}/2) ---")
                draft_analysis_json, error_msg = generate_draft_answer(content, top_5_laws, virtual_law_clause, LLM_MODEL_NAME, unique_law_names, last_suggestion)
                if error_msg:
                    final_analysis_json = {"is_relevant": False, "reason": error_msg}; break
                final_analysis_json = draft_analysis_json
                
                print("í‰ê°€ ì—ì´ì „íŠ¸ê°€ ìƒì„±ëœ ë‹µë³€ì„ ê²€í† í•©ë‹ˆë‹¤...")
                evaluation = evaluate_and_refine_answer(content, top_5_laws, draft_analysis_json, LLM_MODEL_NAME)
                print(f"í‰ê°€ ì ìˆ˜: {evaluation.get('score', 0)}/10 | ğŸ“ í‰ê°€ ìš”ì•½: {evaluation.get('critique', 'N/A')}")
                
                if evaluation.get('is_perfect', False) or evaluation.get('score', 0) >= 9:
                    print("í‰ê°€ ê²°ê³¼ê°€ ìš°ìˆ˜í•˜ì—¬ ë¶„ì„ì„ ìµœì¢… í™•ì •í•©ë‹ˆë‹¤."); break
                else:
                    last_suggestion = evaluation.get('suggestion_for_refinement', "")
                    if not last_suggestion or iter_num == 1:
                        print("ë¶„ì„ì„ ìµœì¢… í™•ì •í•©ë‹ˆë‹¤."); break
                    print(f"ê°œì„  ì§€ì‹œì‚¬í•­: {last_suggestion}")

            _, mapped_law, mapped_article = format_final_report_from_json(final_analysis_json, top_5_laws)
            
            analysis_results.append({
                NEWS_CONTENT_COLUMN: content,
                "mapped_law": mapped_law,
                "mapped_article": mapped_article,
            })
            
        except Exception as e:
            print(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"); traceback.print_exc()
            analysis_results.append({NEWS_CONTENT_COLUMN: content, "mapped_law": "ì˜¤ë¥˜", "mapped_article": f"ì˜¤ë¥˜: {e}"})

        if (index + 1) % 10 == 0:
            pd.DataFrame(analysis_results).to_csv(temp_output_path, index=False, encoding='utf-8-sig')

    # 6. ìµœì¢… ê²°ê³¼ ë³‘í•© ë° ì €ì¥
    print("\n--- ë¶„ì„ ì™„ë£Œ. ì›ë³¸ ë°ì´í„°ì™€ ê²°ê³¼ ë³‘í•© í›„ ì €ì¥ ---")
    try:
        df_analysis = pd.DataFrame(analysis_results)
        final_df = pd.merge(df_news_original, df_analysis, on=NEWS_CONTENT_COLUMN, how='left')
        final_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nìµœì¢… ê²°ê³¼ê°€ '{output_path}' íŒŒì¼ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if temp_output_path.exists():
            os.remove(temp_output_path)
            print(f"ì„ì‹œ íŒŒì¼ '{temp_output_path.name}'ì„(ë¥¼) ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nìµœì¢… íŒŒì¼ ë³‘í•© ë˜ëŠ” ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}"); traceback.print_exc()

if __name__ == "__main__":
    main()
