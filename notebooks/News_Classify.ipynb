{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06d931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                category       MAIN_NAME  count\n",
      "0                         개인정보보호법,정보통신망법    개인정보 유출·보호조치    399\n",
      "1                         개인정보보호법,정보통신망법      개인정보보호법 기타    329\n",
      "2                         개인정보보호법,정보통신망법     법적 분쟁·정치 연루    496\n",
      "3                         개인정보보호법,정보통신망법   온라인·플랫폼 관련 이슈    261\n",
      "4                                  아동복지법     법·제도·사회적 이슈    144\n",
      "5                                  아동복지법  아동 유기·방임·사망 사건    135\n",
      "6                                  아동복지법       아동 학대·성범죄    311\n",
      "7                                  아동복지법        아동복지법 기타    104\n",
      "8   자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법       가상자산·규제정책    442\n",
      "9   자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법     금융사고·소비자 피해    202\n",
      "10  자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법     금융소비자보호법 기타    304\n",
      "11  자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법     시장·기업 관련 사건    453\n",
      "12  자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법      특검정치 연루 사건    267\n",
      "13                               중대재해처벌법         산업재해 사건    513\n",
      "14                               중대재해처벌법      제도·안전관리·정책    409\n",
      "15                               중대재해처벌법          중대시민재해     82\n",
      "16                               중대재해처벌법      중대재해처벌법 기타    149\n"
     ]
    }
   ],
   "source": [
    "# 분류 모델에 사용할 최종 train 파일 확인용\n",
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_csv(\"../data/processed/5000_w_정답라벨_42.csv\") # random_state=42\n",
    "df = pd.read_csv(\"../data/processed/5000_w_정답라벨_final.csv\") # 랜덤 추출 -> json 넘길때 사용한 파일\n",
    "#count = df.groupby([\"predicted_category\", \"predicted_MAIN_NAME\"]).size().reset_index(name=\"count\")\n",
    "#count = df.groupby([\"predicted_category\", \"MAIN_NAME\"]).size().reset_index(name=\"count\")\n",
    "count = df.groupby([\"category\", \"MAIN_NAME\"]).size().reset_index(name=\"count\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cb461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['개인정보보호법,정보통신망법' '아동복지법' '자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법' '중대재해처벌법']\n",
      "['온라인·플랫폼 관련 이슈' '개인정보보호법 기타' '법적 분쟁·정치 연루' '개인정보 유출·보호조치'\n",
      " '아동 유기·방임·사망 사건' '법·제도·사회적 이슈' '아동 학대·성범죄' '아동복지법 기타' '시장·기업 관련 사건'\n",
      " '금융소비자보호법 기타' '가상자산·규제정책' '금융사고·소비자 피해' '특검정치 연루 사건' '중대재해처벌법 기타'\n",
      " '산업재해 사건' '제도·안전관리·정책' '중대시민재해']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:02<00:00, 1643.56 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1580.97 examples/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5116\\36516559.py:358: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `HierTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = HierTrainer(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4473' max='4970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4473/4970 29:50 < 03:19, 2.50 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Law</th>\n",
       "      <th>F1 Law</th>\n",
       "      <th>Accuracy Cat</th>\n",
       "      <th>F1 Cat</th>\n",
       "      <th>F1 Cat Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.513300</td>\n",
       "      <td>0.947238</td>\n",
       "      <td>0.934542</td>\n",
       "      <td>0.932900</td>\n",
       "      <td>0.779456</td>\n",
       "      <td>0.774130</td>\n",
       "      <td>0.782385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.288000</td>\n",
       "      <td>0.925186</td>\n",
       "      <td>0.936556</td>\n",
       "      <td>0.935683</td>\n",
       "      <td>0.819738</td>\n",
       "      <td>0.807962</td>\n",
       "      <td>0.819538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>0.967945</td>\n",
       "      <td>0.944612</td>\n",
       "      <td>0.943785</td>\n",
       "      <td>0.833837</td>\n",
       "      <td>0.826106</td>\n",
       "      <td>0.836902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>1.024702</td>\n",
       "      <td>0.939577</td>\n",
       "      <td>0.936911</td>\n",
       "      <td>0.845921</td>\n",
       "      <td>0.831245</td>\n",
       "      <td>0.846710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>1.099728</td>\n",
       "      <td>0.940584</td>\n",
       "      <td>0.937363</td>\n",
       "      <td>0.841893</td>\n",
       "      <td>0.830014</td>\n",
       "      <td>0.843222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>1.128396</td>\n",
       "      <td>0.939577</td>\n",
       "      <td>0.937381</td>\n",
       "      <td>0.849950</td>\n",
       "      <td>0.838205</td>\n",
       "      <td>0.850365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>1.209476</td>\n",
       "      <td>0.938570</td>\n",
       "      <td>0.936095</td>\n",
       "      <td>0.854985</td>\n",
       "      <td>0.842089</td>\n",
       "      <td>0.855845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>1.266019</td>\n",
       "      <td>0.935549</td>\n",
       "      <td>0.933020</td>\n",
       "      <td>0.853978</td>\n",
       "      <td>0.840771</td>\n",
       "      <td>0.854119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>1.284363</td>\n",
       "      <td>0.936556</td>\n",
       "      <td>0.934742</td>\n",
       "      <td>0.852971</td>\n",
       "      <td>0.836668</td>\n",
       "      <td>0.852312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[완료] 학습 종료. best checkpoint가 로드되었습니다.\n",
      "[저장 완료] Best model을 ./results_hier_weighted/best_model 에 복사했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[검증 결과] {'eval_loss': 1.2094762325286865, 'eval_accuracy_law': 0.9385699899295066, 'eval_f1_law': 0.9360954323686522, 'eval_accuracy_cat': 0.8549848942598187, 'eval_f1_cat': 0.8420890483875625, 'eval_f1_cat_weighted': 0.8558446291546842, 'eval_runtime': 12.8344, 'eval_samples_per_second': 158.013, 'eval_steps_per_second': 9.895, 'epoch': 9.0}\n",
      "\n",
      "[알림] 학습은 '기타' 제외, 평가는 '기타' 제외 샘플로 점수 산정했습니다.\n",
      "[팁] 추후 추론 서비스에서 '기타'를 쓰고 싶다면, 확률 임계값 기반 reject-option을 추가하면 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# train/test\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel,\n",
    "    TrainingArguments, Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import shutil\n",
    "\n",
    "FILE_PATH_WITH_FEATURES = \"../data/processed/5000_w_label.csv\"\n",
    "MODEL_NAME = \"klue/roberta-base\"\n",
    "MAX_LENGTH = 512\n",
    "SLIDING_STRIDE = 256\n",
    "OTHER_NAME = \"기타\"\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 데이터 로더(인코딩 견고)\n",
    "def load_data_with_robust_encodings(file_path):\n",
    "    encodings = ['utf-8', 'euc-kr', 'cp949']\n",
    "    last_err = None\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            return df\n",
    "        except UnicodeDecodeError as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise Exception(f\"파일 로드 실패: {file_path} / 마지막 오류: {last_err}\")\n",
    "\n",
    "# 데이터 로드\n",
    "df = load_data_with_robust_encodings(FILE_PATH_WITH_FEATURES).copy()\n",
    "\n",
    "# content/라벨 필수 컬럼 체크\n",
    "required_cols = {\"content\", \"category\", \"MAIN_NAME\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"필수 컬럼 누락: {missing}\")\n",
    "\n",
    "df[\"content\"] = df[\"content\"].astype(str)\n",
    "df[\"category\"] = df[\"category\"].astype(str)\n",
    "df[\"MAIN_NAME\"] = df[\"MAIN_NAME\"].astype(str)\n",
    "df[\"__index\"] = df.index  # 문서 식별자\n",
    "\n",
    "print(df[\"category\"].unique())\n",
    "print(df[\"MAIN_NAME\"].unique())\n",
    "\n",
    "tmp_main = df[\"MAIN_NAME\"].astype(str)\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=tmp_main\n",
    ")\n",
    "\n",
    "# 학습에서만 '기타' 제외\n",
    "train_df = train_df[train_df[\"MAIN_NAME\"] != OTHER_NAME]\n",
    "\n",
    "# 라벨 맵 (기타 제외 기준)\n",
    "category_list = sorted(train_df[\"category\"].astype(str).unique().tolist())\n",
    "mainname_list = sorted(train_df[\"MAIN_NAME\"].astype(str).unique().tolist())\n",
    "\n",
    "law2id = {l: i for i, l in enumerate(category_list)}\n",
    "cat2id = {c: i for i, c in enumerate(mainname_list)}\n",
    "id2law = {v: k for k, v in law2id.items()}\n",
    "id2cat = {v: k for k, v in cat2id.items()}\n",
    "\n",
    "# 학습 라벨 부여\n",
    "train_df[\"labels_law\"] = train_df[\"category\"].map(law2id)\n",
    "train_df[\"labels_cat\"] = train_df[\"MAIN_NAME\"].map(cat2id)\n",
    "\n",
    "# 평가셋: 공정 비교 위해 '기타' 샘플 제외 권장\n",
    "test_df_eval = test_df[test_df[\"MAIN_NAME\"] != OTHER_NAME].copy()\n",
    "test_df_eval[\"labels_law\"] = test_df_eval[\"category\"].map(law2id)\n",
    "test_df_eval[\"labels_cat\"] = test_df_eval[\"MAIN_NAME\"].map(cat2id)\n",
    "\n",
    "# 혹시 train엔 없고 test에만 있는 클래스가 있으면 NaN → 제거\n",
    "before = len(test_df_eval)\n",
    "test_df_eval = test_df_eval.dropna(subset=[\"labels_law\", \"labels_cat\"])\n",
    "test_df_eval[\"labels_law\"] = test_df_eval[\"labels_law\"].astype(int)\n",
    "test_df_eval[\"labels_cat\"] = test_df_eval[\"labels_cat\"].astype(int)\n",
    "if len(test_df_eval) < before:\n",
    "    print(f\"[주의] 평가셋에서 train에 없는 클래스 {before - len(test_df_eval)}개 제거됨\")\n",
    "\n",
    "num_laws = len(law2id)\n",
    "num_cats = len(cat2id)\n",
    "\n",
    "# 법안-중분류 허용 마스크(기타 제외 기준)\n",
    "allowed_cats_by_law = {\n",
    "    law2id[l]: set(\n",
    "        train_df[train_df[\"category\"] == l][\"labels_cat\"].unique().tolist()\n",
    "    )\n",
    "    for l in category_list\n",
    "}\n",
    "mask_mat = torch.full((num_laws, num_cats), fill_value=-1e9, dtype=torch.float)\n",
    "for law_id, cats in allowed_cats_by_law.items():\n",
    "    for c in cats:\n",
    "        mask_mat[law_id, c] = 0.0\n",
    "\n",
    "\n",
    "# 토크나이저 & 슬라이딩 토큰화\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_with_sliding(batch):\n",
    "    tokenized_batch = tokenizer(\n",
    "        batch[\"content\"],\n",
    "        truncation=True, padding=False, max_length=MAX_LENGTH,\n",
    "        stride=SLIDING_STRIDE, return_overflowing_tokens=True, return_offsets_mapping=False,\n",
    "    )\n",
    "    labels_law_list, labels_cat_list, doc_index_list = [], [], []\n",
    "    # 입력 배치의 각 샘플이 여러 청크로 늘어날 수 있으므로 매핑 사용\n",
    "    for sample_index in tokenized_batch[\"overflow_to_sample_mapping\"]:\n",
    "        labels_law_list.append(batch[\"labels_law\"][sample_index])\n",
    "        labels_cat_list.append(batch[\"labels_cat\"][sample_index])\n",
    "        doc_index_list.append(batch[\"__index\"][sample_index])\n",
    "\n",
    "    tokenized_batch[\"labels_law\"] = labels_law_list\n",
    "    tokenized_batch[\"labels_cat\"] = labels_cat_list\n",
    "    tokenized_batch[\"doc_index\"]  = doc_index_list\n",
    "    del tokenized_batch[\"overflow_to_sample_mapping\"]\n",
    "    return tokenized_batch\n",
    "\n",
    "# Dataset 생성 및 map \n",
    "train_dataset = Dataset.from_pandas(\n",
    "    train_df[[\"content\", \"labels_law\", \"labels_cat\", \"__index\"]].copy()\n",
    ")\n",
    "test_dataset = Dataset.from_pandas(\n",
    "    test_df_eval[[\"content\", \"labels_law\", \"labels_cat\", \"__index\"]].copy()\n",
    ")\n",
    "\n",
    "cols_remove_train = train_dataset.column_names\n",
    "cols_remove_test  = test_dataset.column_names\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_with_sliding, batched=True,\n",
    "    remove_columns=cols_remove_train, batch_size=256\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_with_sliding, batched=True,\n",
    "    remove_columns=cols_remove_test, batch_size=64\n",
    ")\n",
    "\n",
    "\n",
    "# 클래스 가중치(기타 제외 학습 분포 기준)\n",
    "class_weights_cat_np = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(train_df[\"labels_cat\"]),\n",
    "    y=train_df[\"labels_cat\"]\n",
    ")\n",
    "class_weights_cat = torch.tensor(class_weights_cat_np, dtype=torch.float)\n",
    "\n",
    "# 모델 정의\n",
    "class HierarchicalClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_laws, num_cats, mask_mat, class_weights_cat=None):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "\n",
    "       \n",
    "        self.config = self.encoder.config\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.law_head = nn.Linear(hidden, num_laws)\n",
    "        self.cat_head = nn.Linear(hidden, num_cats)\n",
    "\n",
    "        self.register_buffer(\"mask_mat\", mask_mat)\n",
    "        if class_weights_cat is not None:\n",
    "            self.register_buffer(\"class_weights_cat\", class_weights_cat)\n",
    "        else:\n",
    "            self.class_weights_cat = None\n",
    "\n",
    "    \n",
    "    def get_input_embeddings(self):\n",
    "        return self.encoder.get_input_embeddings()\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.encoder.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels_law=None, labels_cat=None, use_predicted_law_for_mask=False):\n",
    "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = enc.last_hidden_state[:, 0]\n",
    "        features = self.dropout(pooled)\n",
    "\n",
    "        law_logits = self.law_head(features)\n",
    "        cat_logits_raw = self.cat_head(features)\n",
    "\n",
    "        if labels_law is not None and not use_predicted_law_for_mask:\n",
    "            law_for_mask = labels_law\n",
    "        else:\n",
    "            law_for_mask = law_logits.argmax(dim=-1)\n",
    "\n",
    "        # mask_selected = self.mask_mat.index_select(dim=0, index=law_for_mask)\n",
    "        mask_selected = self.mask_mat[law_for_mask]  # index_select 대신 직접 인덱싱        \n",
    "        cat_logits = cat_logits_raw + mask_selected\n",
    "\n",
    "        loss = None\n",
    "        if (labels_law is not None) and (labels_cat is not None):\n",
    "            loss_law = nn.CrossEntropyLoss()(law_logits, labels_law)\n",
    "            if self.class_weights_cat is not None:\n",
    "                loss_cat = nn.CrossEntropyLoss(weight=self.class_weights_cat)(cat_logits, labels_cat)\n",
    "            else:\n",
    "                loss_cat = nn.CrossEntropyLoss()(cat_logits, labels_cat)\n",
    "            loss = loss_law + loss_cat \n",
    "\n",
    "        return {\"loss\": loss, \"law_logits\": law_logits, \"cat_logits\": cat_logits, \"cat_logits_raw\": cat_logits_raw}\n",
    "\n",
    "    \n",
    "def custom_collator_dynamic(batch):\n",
    "    text_inputs = [{k: v for k, v in item.items() if k in [\"input_ids\", \"attention_mask\"]} for item in batch]\n",
    "    tokenized_batch = tokenizer.pad(text_inputs, padding=True, return_tensors=\"pt\", max_length=MAX_LENGTH)\n",
    "    return {\n",
    "        **tokenized_batch,\n",
    "        \"labels_law\": torch.tensor([b[\"labels_law\"] for b in batch], dtype=torch.long),\n",
    "        \"labels_cat\": torch.tensor([b[\"labels_cat\"] for b in batch], dtype=torch.long),\n",
    "    }\n",
    "\n",
    "# 커스텀 Trainer (WeightedRandomSampler)\n",
    "class HierTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        labels_cat = np.array(self.train_dataset[\"labels_cat\"])\n",
    "\n",
    "        unique_labels, counts = np.unique(labels_cat, return_counts=True)\n",
    "        class_weights_map = {label: 1.0 / count for label, count in zip(unique_labels, counts)}\n",
    "        sample_weights = np.array([class_weights_map[label] for label in labels_cat])\n",
    "        sample_weights = torch.from_numpy(sample_weights).double()\n",
    "\n",
    "        sampler = torch.utils.data.WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            sampler=sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels_law = inputs.pop(\"labels_law\")\n",
    "        labels_cat = inputs.pop(\"labels_cat\")\n",
    "        outputs = model(**inputs, labels_law=labels_law, labels_cat=labels_cat, use_predicted_law_for_mask=False)\n",
    "        loss = outputs[\"loss\"]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        labels_law = inputs.pop(\"labels_law\")\n",
    "        labels_cat = inputs.pop(\"labels_cat\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels_law=labels_law, labels_cat=labels_cat, use_predicted_law_for_mask=False)\n",
    "        law_logits = outputs[\"law_logits\"].detach().cpu()\n",
    "        cat_logits = outputs[\"cat_logits\"].detach().cpu()\n",
    "        logits = torch.cat([law_logits, cat_logits], dim=1)\n",
    "        labels = torch.stack([labels_law.cpu(), labels_cat.cpu()], dim=1)\n",
    "        loss = outputs[\"loss\"].detach().cpu()\n",
    "        return (loss, logits, labels)\n",
    "\n",
    "\n",
    "# 문서단위 메트릭 (청크 -> 문서 평균 후 Argmax)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    law_logits = logits[:, :num_laws]\n",
    "    cat_logits = logits[:, num_laws:]\n",
    "\n",
    "    law_probs = softmax(torch.tensor(law_logits), dim=-1).numpy()\n",
    "    cat_probs = softmax(torch.tensor(cat_logits), dim=-1).numpy()\n",
    "\n",
    "   \n",
    "    doc_indices = test_dataset[\"doc_index\"]\n",
    "\n",
    "    doc_law_probs, doc_cat_probs = defaultdict(list), defaultdict(list)\n",
    "    for pL, pC, idx in zip(law_probs, cat_probs, doc_indices):\n",
    "        doc_law_probs[idx].append(pL)\n",
    "        doc_cat_probs[idx].append(pC)\n",
    "\n",
    "    final_law_preds, final_cat_preds = [], []\n",
    "    final_law_labels, final_cat_labels = [], []\n",
    "\n",
    "    for idx in doc_law_probs.keys():\n",
    "        avg_law = np.mean(doc_law_probs[idx], axis=0)\n",
    "        avg_cat = np.mean(doc_cat_probs[idx], axis=0)\n",
    "        final_law_preds.append(int(np.argmax(avg_law)))\n",
    "        final_cat_preds.append(int(np.argmax(avg_cat)))\n",
    "\n",
    "        \n",
    "        row = test_df_eval.loc[test_df_eval[\"__index\"] == idx]\n",
    "        if len(row) == 0:\n",
    "            \n",
    "            continue\n",
    "        final_law_labels.append(int(row[\"labels_law\"].iloc[0]))\n",
    "        final_cat_labels.append(int(row[\"labels_cat\"].iloc[0]))\n",
    "\n",
    "    acc_law = accuracy_score(final_law_labels, final_law_preds)\n",
    "    f1_law = f1_score(final_law_labels, final_law_preds, average=\"macro\")\n",
    "    acc_cat = accuracy_score(final_cat_labels, final_cat_preds)\n",
    "    f1_cat = f1_score(final_cat_labels, final_cat_preds, average=\"macro\")\n",
    "    f1_cat_weighted = f1_score(final_cat_labels, final_cat_preds, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy_law\": acc_law,\n",
    "        \"f1_law\": f1_law,\n",
    "        \"accuracy_cat\": acc_cat,\n",
    "        \"f1_cat\": f1_cat,\n",
    "        \"f1_cat_weighted\": f1_cat_weighted\n",
    "    }\n",
    "\n",
    "\n",
    "# 모델/학습 세팅\n",
    "model = HierarchicalClassifier(\n",
    "    MODEL_NAME, num_laws=num_laws, num_cats=num_cats,\n",
    "    mask_mat=mask_mat, class_weights_cat=class_weights_cat\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_hier_weighted\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1.5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),   \n",
    "    logging_dir=\"./logs_hier_weighted\",\n",
    "    logging_steps=50,\n",
    "    metric_for_best_model=\"f1_cat\",\n",
    "    greater_is_better=True,\n",
    "    label_smoothing_factor=0.1,  \n",
    "    dataloader_pin_memory=True,  # GPU 사용 시 메모리 고정\n",
    "    dataloader_drop_last=True,   # 마지막 불완전한 배치 제거         \n",
    ")\n",
    "\n",
    "trainer = HierTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=custom_collator_dynamic,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "train_output = trainer.train()\n",
    "print(\"\\n[완료] 학습 종료. best checkpoint가 로드되었습니다.\")\n",
    "\n",
    "# best checkpoint를 ./results_hier_weighted/best_model 폴더에 복사\n",
    "best_ckpt = trainer.state.best_model_checkpoint\n",
    "best_model_dir = \"./results_hier_weighted/best_model\"\n",
    "\n",
    "if best_ckpt is not None:\n",
    "    shutil.copytree(best_ckpt, best_model_dir, dirs_exist_ok=True)\n",
    "    print(f\"[저장 완료] Best model을 {best_model_dir} 에 복사했습니다.\")\n",
    "else:\n",
    "    print(\"[경고] best_model_checkpoint가 없습니다. TrainingArguments에 load_best_model_at_end=True 가 설정되었는지 확인하세요.\")\n",
    "\n",
    "# 검증 실행\n",
    "eval_output = trainer.evaluate()\n",
    "print(\"[검증 결과]\", eval_output)\n",
    "\n",
    "print(\"\\n[알림] 학습은 '기타' 제외, 평가는 '기타' 제외 샘플로 점수 산정했습니다.\")\n",
    "print(\"[팁] 추후 추론 서비스에서 '기타'를 쓰고 싶다면, 확률 임계값 기반 reject-option을 추가하면 됩니다.\")\n",
    "save_dir = \"./results_hier_weighted\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "label_info = {\n",
    "    \"law2id\": law2id,\n",
    "    \"id2law\": id2law,\n",
    "    \"cat2id\": cat2id,\n",
    "    \"id2cat\": id2cat,\n",
    "    \"mask_mat\": mask_mat.cpu().numpy(),\n",
    "    \"category_list\": category_list,\n",
    "    \"mainname_list\": mainname_list,\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_dir, \"label_mapping.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(label_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54962ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hierarchical 추론:   0%|          | 0/1000 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Hierarchical 추론:   0%|          | 2/1000 [00:00<02:07,  7.83it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Hierarchical 추론: 100%|██████████| 1000/1000 [00:50<00:00, 19.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: ../data/processed/news_predict_result.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict - Hierarchical \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "\n",
    "MODEL_NAME = \"klue/roberta-base\"\n",
    "BEST_MODEL_PATH = \"../models/news_model/model.safetensors\"\n",
    "INPUT_CSV = \"../data/processed/news_pre.csv\"   # content 컬럼 포함한 데이터\n",
    "OUTPUT_CSV = \"../data/processed/news_predict_result.csv\"\n",
    "LABEL_PATH = \"../models/news_model/label_mapping.pkl\"\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "STRIDE = 128\n",
    "BATCH_SIZE = 8\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(LABEL_PATH, \"rb\") as f:\n",
    "    label_info = pickle.load(f)\n",
    "\n",
    "law2id = label_info[\"law2id\"]\n",
    "id2law = label_info[\"id2law\"]\n",
    "cat2id = label_info[\"cat2id\"]\n",
    "id2cat = label_info[\"id2cat\"]\n",
    "mask_mat = torch.tensor(label_info[\"mask_mat\"])\n",
    "category = label_info[\"category_list\"]\n",
    "MAIN_NAME = label_info[\"mainname_list\"]\n",
    "\n",
    "num_laws = len(category)\n",
    "num_cats = len(MAIN_NAME)\n",
    "\n",
    "# 모델 정의\n",
    "class HierarchicalClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_laws, num_cats):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.law_head = nn.Linear(hidden, num_laws)\n",
    "        self.cat_head = nn.Linear(hidden, num_cats)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = enc.last_hidden_state[:, 0]\n",
    "        law_logits = self.law_head(pooled)\n",
    "        cat_logits = self.cat_head(pooled)   # mask 적용 X\n",
    "        return {\"law_logits\": law_logits, \"cat_logits\": cat_logits, \"embedding\": pooled}\n",
    "\n",
    "# 모델 로드\n",
    "model = HierarchicalClassifier(MODEL_NAME, num_laws, num_cats).to(device)\n",
    "state_dict = load_file(BEST_MODEL_PATH, device=str(device))\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "texts = df[\"content\"].astype(str).tolist()\n",
    "\n",
    "# 예측\n",
    "final_law_preds, final_cat_preds = [], []\n",
    "\n",
    "for text in tqdm(texts, desc=\"Hierarchical 추론\"):\n",
    "    tokens = tokenizer(text, truncation=False, padding=False)\n",
    "    input_ids_full = tokens[\"input_ids\"]\n",
    "\n",
    "    # 긴 문서 → 여러 chunk\n",
    "    chunks = []\n",
    "    for start in range(0, len(input_ids_full), MAX_LENGTH-STRIDE):\n",
    "        end = min(start+MAX_LENGTH, len(input_ids_full))\n",
    "        chunks.append(input_ids_full[start:end])\n",
    "        if end == len(input_ids_full): break\n",
    "\n",
    "    # 문서 단위 logits 모으기\n",
    "    all_law_logits, all_cat_logits = [], []\n",
    "\n",
    "    for i in range(0, len(chunks), BATCH_SIZE):\n",
    "        batch_chunks = chunks[i:i+BATCH_SIZE]\n",
    "        enc = tokenizer.pad({\"input_ids\": batch_chunks}, padding=True, return_tensors=\"pt\")\n",
    "        input_ids = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids, attention_mask)\n",
    "            all_law_logits.append(out[\"law_logits\"].cpu())\n",
    "            all_cat_logits.append(out[\"cat_logits\"].cpu())\n",
    "\n",
    "    # 문서 단위 평균 확률\n",
    "    law_avg = torch.mean(torch.cat(all_law_logits, dim=0), dim=0)\n",
    "    law_id = torch.argmax(law_avg).item()   # 최종 대분류 확정\n",
    "\n",
    "    cat_avg = torch.mean(torch.cat(all_cat_logits, dim=0), dim=0)\n",
    "    allowed_cats = (mask_mat[law_id] == 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if len(allowed_cats) > 0:\n",
    "        rel_idx = torch.argmax(cat_avg[allowed_cats]).item()\n",
    "        cat_id = allowed_cats[rel_idx].item()\n",
    "    else:\n",
    "        cat_id = -1 \n",
    "\n",
    "    final_law_preds.append(law_id)\n",
    "    final_cat_preds.append(cat_id)\n",
    "\n",
    "# 결과 저장\n",
    "df[\"predicted_category\"] = [id2law[i] for i in final_law_preds] # 대분류 예측결과\n",
    "df[\"predicted_MAIN_NAME\"] = [id2cat[i] for i in final_cat_preds] # 중분류 예측결과\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"저장 완료: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20117b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      predicted_category predicted_MAIN_NAME  count\n",
      "0                         개인정보보호법,정보통신망법        개인정보 유출·보호조치     78\n",
      "1                         개인정보보호법,정보통신망법          개인정보보호법 기타     80\n",
      "2                         개인정보보호법,정보통신망법         법적 분쟁·정치 연루     86\n",
      "3                         개인정보보호법,정보통신망법       온라인·플랫폼 관련 이슈     58\n",
      "4                                  아동복지법         법·제도·사회적 이슈     31\n",
      "5                                  아동복지법      아동 유기·방임·사망 사건     24\n",
      "6                                  아동복지법           아동 학대·성범죄     57\n",
      "7                                  아동복지법            아동복지법 기타     21\n",
      "8   자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법           가상자산·규제정책     84\n",
      "9   자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법         금융사고·소비자 피해     39\n",
      "10  자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법         금융소비자보호법 기타     50\n",
      "11  자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법         시장·기업 관련 사건     82\n",
      "12  자본시장법,특정금융정보법,전자금융거래법,전자증권법,금융소비자보호법          특검정치 연루 사건     54\n",
      "13                               중대재해처벌법             산업재해 사건    108\n",
      "14                               중대재해처벌법          제도·안전관리·정책    102\n",
      "15                               중대재해처벌법              중대시민재해     11\n",
      "16                               중대재해처벌법          중대재해처벌법 기타     35\n"
     ]
    }
   ],
   "source": [
    "# predict 결과 확인용\n",
    "import pandas as pd\n",
    "# df = pd.read_csv(\"../../data/processed/news_pre.csv\")\n",
    "# count = df.groupby([\"category\"]).size().reset_index(name=\"count\")\n",
    "# print(count)\n",
    "\n",
    "df_pred = pd.read_csv(\"../data/processed/news_predict_result.csv\")\n",
    "count_pred = df_pred.groupby([\"predicted_category\", \"predicted_MAIN_NAME\"]).size().reset_index(name=\"count\")\n",
    "print(count_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
