{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a38eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "\n",
    "# Selenium ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 2. ì„¤ì • (ì´ ë¶€ë¶„ë§Œ ì‚¬ìš©ìì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”) ---\n",
    "# ==============================================================================\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ë° ì´ë¦„ ì„¤ì •\n",
    "#    - í˜„ì¬ .ipynb íŒŒì¼ì´ AIDA/ í´ë”ì— ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "#    - ê²½ë¡œê°€ ë‹¤ë¥´ë©´ ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\n",
    "DATA_DIR = \"data\"\n",
    "PROCESSED_DIR = \"processed\"\n",
    "INPUT_FILENAME = \"news_cluster_w_event_name.csv\"\n",
    "FINAL_OUTPUT_FILENAME = \"Comment_all.csv\"\n",
    "\n",
    "TEMP_OUTPUT_DIR = \"crawled_temp_output\"\n",
    "PROGRESS_FILENAME = \"crawling_progress.json\"\n",
    "\n",
    "# 2. í¬ë¡¤ë§ ê¸°ê°„ ì„¤ì •\n",
    "START_DATE = \"2025.08.14\"\n",
    "END_DATE = (datetime.now() - timedelta(days=1)).strftime(\"%Y.%m.%d\") # ì–´ì œê¹Œì§€\n",
    "\n",
    "# 3. í¬ë¡¤ë§ ì„¸ë¶€ ì„¤ì •\n",
    "MAX_PAGES_PER_QUERY = 10\n",
    "INTERMEDIATE_SAVE_COUNT = 200\n",
    "\n",
    "# 4. ìˆ˜ë™ ì¬ì‹œì‘ (í•„ìš”í•  ë•Œë§Œ ì‚¬ìš©, í‰ì†Œì—ëŠ” None)\n",
    "MANUAL_RESUME_CATEGORY = None\n",
    "MANUAL_RESUME_INDEX = None\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a97277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 3. ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜ ---\n",
    "# ==============================================================================\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"í¬ë¡¤ë§ì— ì‚¬ìš©í•  ì›¹ ë“œë¼ì´ë²„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        print(\"ğŸš€ Selenium WebDriverë¥¼ ì„¤ì •í•©ë‹ˆë‹¤...\")\n",
    "        options = Options()\n",
    "        # options.add_argument(\"--headless\") # ì•ˆì •ì„±ì„ ìœ„í•´ ì‹¤ì œ ë¸Œë¼ìš°ì € ì°½ì„ ë„ì›ë‹ˆë‹¤.\n",
    "        options.add_argument(\"--start-maximized\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--disable-infobars\")\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        print(\"âœ… WebDriver ì„¤ì • ì™„ë£Œ. (ë¸Œë¼ìš°ì € ì°½ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤)\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ WebDriver ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\"); return None\n",
    "\n",
    "def save_progress(progress_file: Path, category: str, query_index: int):\n",
    "    \"\"\"í˜„ì¬ ì§„í–‰ ìƒí™©ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    progress = {'last_completed_category': category, 'last_completed_query_index': query_index}\n",
    "    with open(progress_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(progress, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_progress(progress_file: Path):\n",
    "    \"\"\"ì €ì¥ëœ ì§„í–‰ ìƒí™©ì„ JSON íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        with open(progress_file, 'r', encoding='utf-8') as f:\n",
    "            progress = json.load(f)\n",
    "            print(f\"âœ… ì´ì „ ì§„í–‰ ìƒí™© ë¡œë“œ: {progress}\")\n",
    "            return progress.get('last_completed_category'), progress.get('last_completed_query_index', -1)\n",
    "    except (FileNotFoundError, json.JSONDecodeError, KeyError):\n",
    "        return None, -1\n",
    "\n",
    "def format_naver_date(date_str: str) -> str:\n",
    "    \"\"\"ë„¤ì´ë²„ ë‰´ìŠ¤ ëŒ“ê¸€ì˜ ë‚ ì§œ í˜•ì‹ì„ 'YYYYMMDDHHMMSS' ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    now = datetime.now()\n",
    "    date_str = str(date_str).strip()\n",
    "    try:\n",
    "        if \"ë°©ê¸ˆ ì „\" in date_str or \"ì´ˆ ì „\" in date_str: return str(now.strftime('%Y%m%d%H%M%S'))\n",
    "        if \"ë¶„ ì „\" in date_str:\n",
    "            min_ago = int(re.search(r'\\d+', date_str).group())\n",
    "            return str((now - timedelta(minutes=min_ago)).strftime('%Y%m%d%H%M%S'))\n",
    "        if \"ì‹œê°„ ì „\" in date_str:\n",
    "            hour_ago = int(re.search(r'\\d+', date_str).group())\n",
    "            return str((now - timedelta(hours=hour_ago)).strftime('%Y%m%d%H%M%S'))\n",
    "        if \"ì–´ì œ\" in date_str:\n",
    "            target_time = now - timedelta(days=1)\n",
    "            if time_match := re.search(r'(\\d{2}):(\\d{2})', date_str):\n",
    "                hour, minute = map(int, time_match.groups())\n",
    "                target_time = target_time.replace(hour=hour, minute=minute, second=0)\n",
    "            return str(target_time.strftime('%Y%m%d%H%M%S'))\n",
    "        if re.match(r'\\d{4}\\.\\d{2}\\.\\d{2}\\. \\d{2}:\\d{2}', date_str):\n",
    "            return str(datetime.strptime(date_str, '%Y.%m.%d. %H:%M').strftime('%Y%m%d%H%M00'))\n",
    "        if re.match(r'\\d{4}\\.\\d{2}\\.\\d{2}\\.', date_str):\n",
    "            return str(datetime.strptime(date_str, '%Y.%m.%d.').strftime('%Y%m%d000000'))\n",
    "    except (ValueError, AttributeError): pass\n",
    "    return str(now.strftime('%Y%m%d%H%M%S'))\n",
    "\n",
    "def crawl_article_content(article_url: str, headers: dict) -> str:\n",
    "    \"\"\"ê¸°ì‚¬ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(article_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        content_area = soup.select_one('#dic_area, #articeBody, #newsct_article')\n",
    "        if content_area:\n",
    "            for tag in content_area.select('script, style, .reporter_area'): tag.decompose()\n",
    "            return content_area.get_text(strip=True, separator='\\n')\n",
    "        return \"\"\n",
    "    except Exception: return \"\"\n",
    "\n",
    "def crawl_comments(driver: webdriver.Chrome, article_url: str) -> list:\n",
    "    \"\"\"ëŒ“ê¸€ ì „ìš© URLë¡œ ì ‘ì†í•˜ì—¬ ëŒ“ê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not driver: return []\n",
    "    match = re.search(r'/article/(\\d+)/(\\d+)', article_url)\n",
    "    if not match: return []\n",
    "    \n",
    "    press_id, article_id = match.groups()\n",
    "    comment_page_url = f\"https://n.news.naver.com/article/comment/{press_id}/{article_id}\"\n",
    "    \n",
    "    try:\n",
    "        driver.get(comment_page_url)\n",
    "        try:\n",
    "            comment_count_element = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"span.u_cbox_count\")))\n",
    "            comment_count = int(re.sub(r'[^0-9]', '', comment_count_element.text))\n",
    "            if comment_count == 0: return []\n",
    "        except (TimeoutException, ValueError): return []\n",
    "            \n",
    "        click_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                more_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.u_cbox_btn_more\")))\n",
    "                driver.execute_script(\"arguments[0].click();\", more_button); click_count += 1\n",
    "                print(f\"    -> 'ë”ë³´ê¸°' ({click_count}íšŒ)\", end='\\r'); time.sleep(0.5)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                if click_count > 0: print(\"\\n    -> 'ë”ë³´ê¸°' ì™„ë£Œ.\")\n",
    "                break\n",
    "                \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        contents = soup.select(\"span.u_cbox_contents\")\n",
    "        dates = soup.select(\"span.u_cbox_date\")\n",
    "        \n",
    "        return [{'comment_content': c.get_text(strip=True), 'comment_date': format_naver_date(d.get_text(strip=True))} for c, d in zip(contents, dates)]\n",
    "    except Exception as e:\n",
    "        print(f\"\\n    âŒ ëŒ“ê¸€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}\"); return []\n",
    "\n",
    "def save_to_csv(data_list: list, file_path: Path):\n",
    "    \"\"\"ë°ì´í„°ë¥¼ CSV íŒŒì¼ì— ì¶”ê°€ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not data_list: return\n",
    "    write_header = not file_path.exists()\n",
    "    try:\n",
    "        with open(file_path, 'a', newline='', encoding='utf-8-sig') as f:\n",
    "            fieldnames = ['category', 'main_name', 'query', 'title', 'url', 'content', 'comment_content', 'comment_date']\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            if write_header: writer.writeheader()\n",
    "            writer.writerows(data_list)\n",
    "    except Exception as e: print(f\"\\nâŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
    "\n",
    "def finalize_results(crawled_dir: Path, final_output_path: Path):\n",
    "    \"\"\"ì¤‘ê°„ ì €ì¥ëœ ëª¨ë“  CSVë¥¼ í•©ì¹˜ê³ , ì¤‘ë³µ ì œê±° í›„ ì„ì‹œ í´ë”ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    print(\"\\n\\n--- [ìµœì¢… ë‹¨ê³„] ê²°ê³¼ íŒŒì¼ ë³‘í•© ë° ì •ë¦¬ ---\")\n",
    "    \n",
    "    csv_files = [f for f in os.listdir(crawled_dir) if f.startswith('crawled_') and f.endswith('.csv')]\n",
    "    if not csv_files: print(\"-> ë³‘í•©í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\"); return\n",
    "\n",
    "    df_list = [pd.read_csv(crawled_dir / file, dtype={'comment_date': str}) for file in csv_files]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    print(f\"-> ì¤‘ë³µ ì œê±° ì „: {len(combined_df)} ê±´\")\n",
    "    deduplicated_df = combined_df.drop_duplicates()\n",
    "    print(f\"-> ì¤‘ë³µ ì œê±° í›„: {len(deduplicated_df)} ê±´\")\n",
    "    \n",
    "    final_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    deduplicated_df.to_csv(final_output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… ìµœì¢… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: '{final_output_path}'\")\n",
    "    \n",
    "    try:\n",
    "        shutil.rmtree(crawled_dir)\n",
    "        print(f\"âœ… ì„ì‹œ í´ë” '{crawled_dir.name}' ì‚­ì œ ì™„ë£Œ.\")\n",
    "    except Exception as e: print(f\"â—ï¸ ì„ì‹œ í´ë” ì‚­ì œ ì˜¤ë¥˜: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c46c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 4. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---\n",
    "# ==============================================================================\n",
    "# (Jupyter Notebookì—ì„œëŠ” ì´ ë¶€ë¶„ì„ í•˜ë‚˜ì˜ ì…€ë¡œ ë¶„ë¦¬í•˜ì—¬ ì‹¤í–‰í•´ë„ ì¢‹ìŠµë‹ˆë‹¤)\n",
    "\n",
    "# --- ê²½ë¡œ ì¤€ë¹„ ---\n",
    "base_path = Path(\".\") # í˜„ì¬ .ipynb íŒŒì¼ì´ ìˆëŠ” ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n",
    "input_path = base_path / DATA_DIR / PROCESSED_DIR / INPUT_FILENAME\n",
    "temp_output_dir = base_path / TEMP_OUTPUT_DIR\n",
    "final_output_path = base_path / DATA_DIR / PROCESSED_DIR / FINAL_OUTPUT_FILENAME\n",
    "progress_file = temp_output_dir / PROGRESS_FILENAME\n",
    "temp_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- ì…ë ¥ íŒŒì¼ ë¡œë“œ ---\n",
    "try:\n",
    "    df = pd.read_csv(input_path)\n",
    "    categories = df['predicted_category'].dropna().unique()\n",
    "    print(f\"âœ… '{INPUT_FILENAME}' íŒŒì¼ì—ì„œ {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë¡œë“œ ì™„ë£Œ.\")\n",
    "except (FileNotFoundError, KeyError) as e:\n",
    "    print(f\"âŒ ì…ë ¥ íŒŒì¼ ì˜¤ë¥˜: {e}\")\n",
    "    df = None # ì˜¤ë¥˜ ë°œìƒ ì‹œ dfë¥¼ Noneìœ¼ë¡œ ì„¤ì •\n",
    "\n",
    "# --- ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ ---\n",
    "if df is not None:\n",
    "    if MANUAL_RESUME_CATEGORY and MANUAL_RESUME_INDEX is not None:\n",
    "        save_progress(progress_file, MANUAL_RESUME_CATEGORY, MANUAL_RESUME_INDEX)\n",
    "        print(f\"âœ… ìˆ˜ë™ ì¬ì‹œì‘ ì§€ì  ì„¤ì •ë¨: '{MANUAL_RESUME_CATEGORY}'ì˜ {MANUAL_RESUME_INDEX + 1}ë²ˆì§¸ë¶€í„°\")\n",
    "\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    if driver:\n",
    "        completed_successfully = False\n",
    "        try:\n",
    "            start_category, start_index = load_progress(progress_file)\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0;...)'} # User-Agent\n",
    "            can_start_crawling = not start_category\n",
    "\n",
    "            for category in categories:\n",
    "                if not can_start_crawling and category != start_category:\n",
    "                    print(f\"â© ì¹´í…Œê³ ë¦¬ [{category}] ê±´ë„ˆë›°ê¸°\")\n",
    "                    continue\n",
    "                can_start_crawling = True\n",
    "                \n",
    "                output_csv_path = temp_output_dir / f\"crawled_{category}.csv\"\n",
    "                queries = df[df['predicted_category'] == category][['cluster_event_name', 'predicted_MAIN_NAME']].drop_duplicates().to_dict('records')\n",
    "                temp_results = []\n",
    "                current_start_index = start_index if category == start_category else 0\n",
    "                \n",
    "                for i in range(current_start_index, len(queries)):\n",
    "                    item = queries[i]\n",
    "                    query, main_name = item['cluster_event_name'], item['predicted_MAIN_NAME']\n",
    "                    print(f\"\\nâ–¶ [{category}] {i+1}/{len(queries)} - \\\"{query}\\\"\")\n",
    "\n",
    "                    for page in range(1, MAX_PAGES_PER_QUERY + 1):\n",
    "                        url = f\"https://search.naver.com/search.naver?where=news&query={quote_plus(query)}&start={(page-1)*10+1}&pd=3&ds={START_DATE}&de={END_DATE}\"\n",
    "                        try:\n",
    "                            response = requests.get(url, headers=headers)\n",
    "                            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                            news_containers = [h.parent for h in soup.select('div[data-sds-comp=\"Profile\"]') if h.parent]\n",
    "                            if not news_containers: break\n",
    "\n",
    "                            for container in news_containers:\n",
    "                                title_tag = container.select_one('a[data-heatmap-target=\".tit\"]')\n",
    "                                naver_news_tag = container.select_one('a[data-heatmap-target=\".nav\"]')\n",
    "                                if not (title_tag and naver_news_tag and \"n.news.naver.com\" in naver_news_tag['href']): continue\n",
    "                                \n",
    "                                news_url = naver_news_tag['href']\n",
    "                                title = title_tag.get_text(strip=True)\n",
    "                                content = crawl_article_content(news_url, headers)\n",
    "                                comments = crawl_comments(driver, news_url)\n",
    "                                print(f\"  - ê¸°ì‚¬: {title[:30]}... (ëŒ“ê¸€: {len(comments)}ê°œ)\")\n",
    "                                \n",
    "                                for comment in comments:\n",
    "                                    temp_results.append({\n",
    "                                        'category': category, 'main_name': main_name, 'query': query,\n",
    "                                        'title': title, 'url': news_url, 'content': content,\n",
    "                                        'comment_content': comment['comment_content'], 'comment_date': comment['comment_date']\n",
    "                                    })\n",
    "                                \n",
    "                                if len(temp_results) >= INTERMEDIATE_SAVE_COUNT:\n",
    "                                    save_to_csv(temp_results, output_csv_path); temp_results.clear()\n",
    "                                time.sleep(0.5)\n",
    "                        except Exception as e: print(f\"  âŒ í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "                        time.sleep(1)\n",
    "                    \n",
    "                    save_progress(progress_file, category, i)\n",
    "                \n",
    "                save_to_csv(temp_results, output_csv_path)\n",
    "                start_index = 0\n",
    "            \n",
    "            completed_successfully = True\n",
    "        finally:\n",
    "            if driver: driver.quit()\n",
    "            if completed_successfully and progress_file.exists(): os.remove(progress_file)\n",
    "\n",
    "        # --- ìµœì¢… ê²°ê³¼ íŒŒì¼ ì •ë¦¬ ---\n",
    "        finalize_results(temp_output_dir, final_output_path)\n",
    "        \n",
    "        print(\"\\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
