{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841b8d9d",
   "metadata": {},
   "source": [
    "#### í´ëŸ¬ìŠ¤í„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74507eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ ì €ì¥ëœ ì„ë² ë”© íŒŒì¼ ë¡œë“œ: '../../data/processed\\full_article_embeddings.npz'\n",
      "â–¶ ì„ë² ë”© ì¤€ë¹„ ì™„ë£Œ.\n",
      "\n",
      "ë‹¨ê³„ 1: MAIN_NAMEë³„ ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§, ë³‘í•©, ì¬í• ë‹¹ ì‹œì‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MAIN_NAMEë³„ ì²˜ë¦¬:   0%|          | 0/17 [00:00<?, ?it/s]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:   6%|â–Œ         | 1/17 [00:28<07:40, 28.80s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  12%|â–ˆâ–        | 2/17 [00:34<03:46, 15.08s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  18%|â–ˆâ–Š        | 3/17 [00:37<02:14,  9.62s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  24%|â–ˆâ–ˆâ–       | 4/17 [00:40<01:33,  7.17s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  29%|â–ˆâ–ˆâ–‰       | 5/17 [00:43<01:04,  5.40s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 6/17 [00:45<00:46,  4.23s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 7/17 [01:02<01:25,  8.51s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 8/17 [01:16<01:31, 10.15s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 9/17 [01:23<01:15,  9.38s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10/17 [01:24<00:47,  6.83s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 11/17 [01:30<00:38,  6.43s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 12/17 [01:30<00:23,  4.63s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 13/17 [01:37<00:20,  5.21s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 14/17 [01:45<00:18,  6.08s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 15/17 [01:46<00:08,  4.46s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [01:50<00:04,  4.31s/it]C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "MAIN_NAMEë³„ ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [01:55<00:00,  6.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ë‹¨ê³„ 1 ì™„ë£Œ ---\n",
      "\n",
      "ë‹¨ê³„ 2: ìµœì¢… ID ë° ì´ë¦„ ë¶€ì—¬ ì‹œì‘\n",
      "--- ë‹¨ê³„ 2 ì™„ë£Œ ---\n",
      "\n",
      "ë‹¨ê³„ 3: ìµœì¢… ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥ ì‹œì‘\n",
      "\n",
      "â–¶ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "  - ìµœì¢… ë…¸ì´ì¦ˆ: 1640ê°œ\n",
      "  - ìµœì¢… ê²°ê³¼ê°€ '../../data/processed\\news_cluster.csv' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "INPUT_CSV_PATH = \"../data/processed/news_predict_result.csv\"\n",
    "OUTPUT_DIR = \"../data/processed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FINAL_OUTPUT_CSV = os.path.join(OUTPUT_DIR, \"news_cluster.csv\")\n",
    "EMBEDDING_FILE_PATH = os.path.join(OUTPUT_DIR, \"full_article_embeddings.npz\")\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "MERGE_THRESHOLD = 0.80\n",
    "REASSIGN_THRESHOLD = 0.70 \n",
    "\n",
    "def get_representative_doc(vectors, texts_in_cluster):\n",
    "    \"\"\"ëŒ€í‘œ ë¬¸ì¥ìœ¼ë¡œ contentë¥¼ ë°˜í™˜\"\"\"\n",
    "    if not texts_in_cluster or len(vectors) == 0: return None\n",
    "    centroid = np.mean(vectors, axis=0)\n",
    "    distances = cosine_distances([centroid], vectors)[0]\n",
    "    return texts_in_cluster[np.argmin(distances)]\n",
    "\n",
    "def merge_clusters(df_group, group_embeddings, merge_threshold, cluster_col='cluster_id'):\n",
    "    valid_clusters = df_group[df_group[cluster_col] != -1]\n",
    "    if valid_clusters.empty or valid_clusters[cluster_col].nunique() < 2:\n",
    "        return df_group\n",
    "    \n",
    "    centroids = {\n",
    "        cid: np.mean(group_embeddings[valid_clusters[valid_clusters[cluster_col] == cid].index], axis=0)\n",
    "        for cid in valid_clusters[cluster_col].unique()\n",
    "    }\n",
    "    cids, vectors = list(centroids.keys()), np.array(list(centroids.values()))\n",
    "    sim_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    merge_map = {}\n",
    "    temp_id_counter = -2\n",
    "    for i in range(len(cids)):\n",
    "        for j in range(i + 1, len(cids)):\n",
    "            if sim_matrix[i, j] >= merge_threshold:\n",
    "                id1, id2 = cids[i], cids[j]\n",
    "                g1, g2 = merge_map.get(id1, id1), merge_map.get(id2, id2)\n",
    "                if g1 != g2:\n",
    "                    new_id = min(g1, g2) if isinstance(g1, int) and g1 < 0 else temp_id_counter\n",
    "                    for k, v in merge_map.items():\n",
    "                        if v in [g1, g2]: merge_map[k] = new_id\n",
    "                    merge_map.update({id1: new_id, id2: new_id})\n",
    "                    if new_id == temp_id_counter: temp_id_counter -= 1\n",
    "    \n",
    "    if not merge_map: return df_group\n",
    "    \n",
    "    df_group[cluster_col] = df_group[cluster_col].apply(lambda x: merge_map.get(x, x) if x != -1 else x)\n",
    "    merged_ids = sorted(df_group[(df_group[cluster_col] < 0) & (df_group[cluster_col] != -1)][cluster_col].unique())\n",
    "    \n",
    "    if merged_ids:\n",
    "        max_id = df_group[df_group[cluster_col] >= 0][cluster_col].max() if not df_group[df_group[cluster_col] >= 0].empty else -1\n",
    "        id_remap = {old: max_id + i + 1 for i, old in enumerate(merged_ids)}\n",
    "        df_group[cluster_col] = df_group[cluster_col].replace(id_remap)\n",
    "        \n",
    "    return df_group\n",
    "\n",
    "# ë©”ì¸ ì‹¤í–‰ ë¡œì§\n",
    "def main(df):\n",
    "    if os.path.exists(EMBEDDING_FILE_PATH):\n",
    "        print(f\"â–¶ ì €ì¥ëœ ì„ë² ë”© íŒŒì¼ ë¡œë“œ: '{EMBEDDING_FILE_PATH}'\")\n",
    "        embeddings = np.load(EMBEDDING_FILE_PATH)['embeddings']\n",
    "    else:\n",
    "        print(\"â–¶ ì„ë² ë”© ìƒì„±...\")\n",
    "        model = SentenceTransformer(\"jhgan/ko-sbert-sts\")\n",
    "        embeddings = model.encode(df[\"content\"].fillna(\"\").tolist(), show_progress_bar=True)\n",
    "        np.savez_compressed(EMBEDDING_FILE_PATH, embeddings=embeddings)\n",
    "    print(\"â–¶ ì„ë² ë”© ì¤€ë¹„ ì™„ë£Œ.\")\n",
    "\n",
    "    print(\"\\në‹¨ê³„ 1: MAIN_NAMEë³„ ë¡œì»¬ í´ëŸ¬ìŠ¤í„°ë§, ë³‘í•©, ì¬í• ë‹¹ ì‹œì‘\")\n",
    "    processed_groups = []\n",
    "    \n",
    "    for main_name, group in tqdm(df.groupby(\"predicted_MAIN_NAME\"), desc=\"MAIN_NAMEë³„ ì²˜ë¦¬\"):\n",
    "        group = group.copy()\n",
    "        original_indices = group.index.tolist()\n",
    "        group.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        group_embeddings_slice = embeddings[original_indices]\n",
    "        \n",
    "        n_articles = len(group)\n",
    "        min_cluster_size = max(2, min(int(n_articles * 0.002), 50))\n",
    "        n_neighbors = max(5, min(min_cluster_size * 2, 50))\n",
    "        min_samples = max(3, min(int(min_cluster_size * 0.5), 15))\n",
    "        \n",
    "        reduced = umap.UMAP(n_neighbors=n_neighbors, n_components=15, metric=\"cosine\", random_state=42).fit_transform(group_embeddings_slice)\n",
    "        group['cluster_id'] = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples).fit_predict(reduced)\n",
    "        \n",
    "        group = merge_clusters(group, group_embeddings_slice, MERGE_THRESHOLD)\n",
    "        \n",
    "        noise_mask = group['cluster_id'] == -1\n",
    "        if noise_mask.sum() > 0 and (~noise_mask).sum() > 0:\n",
    "            valid_clusters = group[~noise_mask]\n",
    "            centroids = {cid: np.mean(group_embeddings_slice[valid_clusters[valid_clusters['cluster_id'] == cid].index], 0) for cid in valid_clusters['cluster_id'].unique()}\n",
    "            cids, vectors = list(centroids.keys()), np.array(list(centroids.values()))\n",
    "            \n",
    "            for idx in group[noise_mask].index: \n",
    "                sims = cosine_similarity(group_embeddings_slice[idx].reshape(1, -1), vectors)[0]\n",
    "                best = np.argmax(sims)\n",
    "                if sims[best] >= REASSIGN_THRESHOLD:\n",
    "                    assigned_cid = cids[best]\n",
    "                    group.loc[idx, 'cluster_id'] = assigned_cid\n",
    "        \n",
    "        group.index = original_indices\n",
    "        processed_groups.append(group)\n",
    "\n",
    "    df_processed = pd.concat(processed_groups)\n",
    "    print(\"--- ë‹¨ê³„ 1 ì™„ë£Œ ---\")\n",
    "\n",
    "    print(\"\\në‹¨ê³„ 2: ìµœì¢… ID ë° ì´ë¦„ ë¶€ì—¬ ì‹œì‘\")\n",
    "    df_processed['cluster_name'] = \"\"\n",
    "    valid_mask = df_processed['cluster_id'] != -1\n",
    "    df_processed.loc[valid_mask, 'cluster_name'] = df_processed[valid_mask].apply(\n",
    "        lambda row: f\"{row['predicted_MAIN_NAME']}_{int(row['cluster_id'])}\", axis=1)\n",
    "    \n",
    "    noise_mask = df_processed['cluster_id'] == -1\n",
    "    if noise_mask.sum() > 0:\n",
    "        noise_numbers = df_processed[noise_mask].groupby('predicted_MAIN_NAME').cumcount() + 1\n",
    "        df_processed.loc[noise_mask, 'cluster_name'] = df_processed[noise_mask]['predicted_MAIN_NAME'] + '_noise_' + noise_numbers.astype(str)\n",
    "\n",
    "    unique_cluster_names = sorted(df_processed[valid_mask]['cluster_name'].unique())\n",
    "    name_to_id_map = {name: i for i, name in enumerate(unique_cluster_names)}\n",
    "    df_processed['final_cluster_id'] = df_processed['cluster_name'].map(name_to_id_map).fillna(-1).astype(int)\n",
    "    print(\"--- ë‹¨ê³„ 2 ì™„ë£Œ ---\")\n",
    "    \n",
    "    print(\"\\në‹¨ê³„ 3: ìµœì¢… ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥ ì‹œì‘\")\n",
    "    final_reps_map = {}\n",
    "    valid_clusters_final = df_processed[df_processed['final_cluster_id'] != -1]\n",
    "    if not valid_clusters_final.empty:\n",
    "         # ëŒ€í‘œ ë¬¸ì¥ì„ ê³„ì‚°í•  ë•Œ content ì‚¬ìš©\n",
    "         final_reps_map = {\n",
    "            cname: get_representative_doc(embeddings[g.index], g[\"content\"].tolist()) \n",
    "            for cname, g in valid_clusters_final.groupby('cluster_name')\n",
    "        }\n",
    "    df_processed['representative'] = df_processed['cluster_name'].map(final_reps_map)\n",
    "    df_processed.loc[df_processed['representative'].isnull(), 'representative'] = df_processed['content'] \n",
    "\n",
    "    final_noise_count = (df_processed['final_cluster_id'] == -1).sum()\n",
    "    \n",
    "    df_processed.drop(columns=['cluster_id', 'category'], inplace=True, errors='ignore')\n",
    "    df_processed.rename(columns={\n",
    "        'final_cluster_id': 'cluster_id', \n",
    "        'predicted_category': 'category',\n",
    "        'predicted_MAIN_NAME': 'MAIN_NAME'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df_processed = df_processed.sort_values(by=['MAIN_NAME', 'cluster_id'])\n",
    "    \n",
    "    final_cols = ['category', 'MAIN_NAME', 'date', 'cluster_id', 'cluster_name', 'representative', 'title', 'content', 'url']\n",
    "    other_cols = [col for col in df_processed.columns if col not in final_cols and col != 'index']\n",
    "    df_processed = df_processed[final_cols + other_cols]\n",
    "\n",
    "    df_processed.to_csv(FINAL_OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(f\"\\nâ–¶ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"  - ìµœì¢… ë…¸ì´ì¦ˆ: {final_noise_count}ê°œ\")\n",
    "    print(f\"  - ìµœì¢… ê²°ê³¼ê°€ '{FINAL_OUTPUT_CSV}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df_initial = pd.read_csv(INPUT_CSV_PATH)\n",
    "        main(df_initial)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ì˜¤ë¥˜: '{INPUT_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31288a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì „ì²´ ìš”ì•½\n",
      "ì´ ê¸°ì‚¬ ìˆ˜\t\t: 27969ê°œ\n",
      "ìƒì„±ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ (ë…¸ì´ì¦ˆ ì œì™¸)\t: 1135ê°œ\n",
      "ë…¸ì´ì¦ˆë¡œ ë¶„ë¥˜ëœ ê¸°ì‚¬ ìˆ˜\t: 1640ê°œ\n",
      "ë…¸ì´ì¦ˆ ë¹„ìœ¨\t\t: 5.86%\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "MAIN_NAMEë³„ ìƒì„¸ ìš”ì•½\n",
      "         MAIN_NAME  ì´ ê¸°ì‚¬ ìˆ˜  í´ëŸ¬ìŠ¤í„° ìˆ˜  ë…¸ì´ì¦ˆ ìˆ˜ ë…¸ì´ì¦ˆ ë¹„ìœ¨ (%)\n",
      "0        ê°€ìƒìì‚°Â·ê·œì œì •ì±…    3653      40    462      12.65\n",
      "1      ë²•ì  ë¶„ìŸÂ·ì •ì¹˜ ì—°ë£¨    3206      91    115       3.59\n",
      "2          ì‚°ì—…ì¬í•´ ì‚¬ê±´    2825     107    148       5.24\n",
      "3       ì œë„Â·ì•ˆì „ê´€ë¦¬Â·ì •ì±…    2085      90    149       7.15\n",
      "4      ì‹œì¥Â·ê¸°ì—… ê´€ë ¨ ì‚¬ê±´    2039      75     60       2.94\n",
      "5    ì˜¨ë¼ì¸Â·í”Œë«í¼ ê´€ë ¨ ì´ìŠˆ    1891      76    113       5.98\n",
      "6        ì•„ë™ í•™ëŒ€Â·ì„±ë²”ì£„    1715      76     23       1.34\n",
      "7     ê°œì¸ì •ë³´ ìœ ì¶œÂ·ë³´í˜¸ì¡°ì¹˜    1691      65     50       2.96\n",
      "8       íŠ¹ê²€ì •ì¹˜ ì—°ë£¨ ì‚¬ê±´    1671      50     62       3.71\n",
      "9       ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²• ê¸°íƒ€    1415     103    176      12.44\n",
      "10     ê¸ˆìœµì‚¬ê³ Â·ì†Œë¹„ì í”¼í•´    1300      75     37       2.85\n",
      "11      ê°œì¸ì •ë³´ë³´í˜¸ë²• ê¸°íƒ€    1228      93     79       6.43\n",
      "12     ê¸ˆìœµì†Œë¹„ìë³´í˜¸ë²• ê¸°íƒ€     975      68     74       7.59\n",
      "13     ë²•Â·ì œë„Â·ì‚¬íšŒì  ì´ìŠˆ     906      53     52       5.74\n",
      "14  ì•„ë™ ìœ ê¸°Â·ë°©ì„Â·ì‚¬ë§ ì‚¬ê±´     608      29      7       1.15\n",
      "15          ì¤‘ëŒ€ì‹œë¯¼ì¬í•´     437      15     12       2.75\n",
      "16        ì•„ë™ë³µì§€ë²• ê¸°íƒ€     324      29     21       6.48\n",
      "============================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í™•ì¸ìš©\n",
    "import pandas as pd\n",
    "FILE_PATH = \"../data/processed/news_cluster.csv\"\n",
    "CLUSTER_ID_COL = \"cluster_id\"         # ë…¸ì´ì¦ˆ(-1) íŒë³„ì„ ìœ„í•´ í´ëŸ¬ìŠ¤í„° ID ì»¬ëŸ¼ ì¶”ê°€\n",
    "CLUSTER_NAME_COL = \"cluster_name\"\n",
    "TITLE_COL = \"title\"\n",
    "REPRESENTATIVE_COL = \"representative\" \n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ì˜¤ë¥˜: '{FILE_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "    exit()\n",
    "\n",
    "# í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "required_cols = [CLUSTER_ID_COL, CLUSTER_NAME_COL, TITLE_COL, REPRESENTATIVE_COL]\n",
    "if not all(col in df.columns for col in required_cols):\n",
    "    print(f\"ì˜¤ë¥˜: íŒŒì¼ì— '{', '.join(required_cols)}' ì»¬ëŸ¼ì´ ëª¨ë‘ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "    print(f\"    í˜„ì¬ íŒŒì¼ì— ìˆëŠ” ì»¬ëŸ¼: {df.columns.tolist()}\")\n",
    "    exit()\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ìš”ì•½ ì •ë³´ ì¶œë ¥\n",
    "total_articles = len(df)\n",
    "noise_articles_count = (df[CLUSTER_ID_COL] == -1).sum()\n",
    "total_valid_clusters = df[df[CLUSTER_ID_COL] != -1][CLUSTER_NAME_COL].nunique()\n",
    "noise_ratio = (noise_articles_count / total_articles) * 100 if total_articles > 0 else 0\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì „ì²´ ìš”ì•½\")\n",
    "print(f\"ì´ ê¸°ì‚¬ ìˆ˜\\t\\t: {total_articles}ê°œ\")\n",
    "print(f\"ìƒì„±ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜ (ë…¸ì´ì¦ˆ ì œì™¸)\\t: {total_valid_clusters}ê°œ\")\n",
    "print(f\"ë…¸ì´ì¦ˆë¡œ ë¶„ë¥˜ëœ ê¸°ì‚¬ ìˆ˜\\t: {noise_articles_count}ê°œ\")\n",
    "print(f\"ë…¸ì´ì¦ˆ ë¹„ìœ¨\\t\\t: {noise_ratio:.2f}%\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# MAIN_NAMEë³„ ìš”ì•½ ì •ë³´ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MAIN_NAMEë³„ ìƒì„¸ ìš”ì•½\")\n",
    "\n",
    "summary_list = []\n",
    "for name, group in df.groupby('MAIN_NAME'):\n",
    "    total = len(group)\n",
    "    noise = (group[CLUSTER_ID_COL] == -1).sum()\n",
    "    clusters = group[group[CLUSTER_ID_COL] != -1][CLUSTER_NAME_COL].nunique()\n",
    "    ratio = (noise / total * 100) if total > 0 else 0\n",
    "    summary_list.append({\n",
    "        'MAIN_NAME': name,\n",
    "        'ì´ ê¸°ì‚¬ ìˆ˜': total,\n",
    "        'í´ëŸ¬ìŠ¤í„° ìˆ˜': clusters,\n",
    "        'ë…¸ì´ì¦ˆ ìˆ˜': noise,\n",
    "        'ë…¸ì´ì¦ˆ ë¹„ìœ¨ (%)': f\"{ratio:.2f}\"\n",
    "    })\n",
    "\n",
    "if summary_list:\n",
    "    summary_df = pd.DataFrame(summary_list)\n",
    "    # ì´ ê¸°ì‚¬ ìˆ˜ê°€ ë§ì€ ìˆœì„œë¡œ ì •ë ¬\n",
    "    summary_df = summary_df.sort_values(by='ì´ ê¸°ì‚¬ ìˆ˜', ascending=False).reset_index(drop=True)\n",
    "    print(summary_df.to_string()) \n",
    "else:\n",
    "    print(\"ìš”ì•½í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"=\"*60, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6fcd9",
   "metadata": {},
   "source": [
    "#### í´ëŸ¬ìŠ¤í„°ë§ ì´í›„ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad26f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 20ê±´ ë°ì´í„° ì‚¬ìš©\n",
      "\n",
      "=== í´ëŸ¬ìŠ¤í„° ì†Œë¶„ë¥˜ëª… ìƒì„± ê²°ê³¼ ===\n",
      "[ê°€ìƒìì‚°Â·ê·œì œì •ì±…_76] â†’ êµ­íšŒ ìƒì†ì„¸ ê°œì • ë…¼ì˜\n",
      "[ê°€ìƒìì‚°Â·ê·œì œì •ì±…_noise_274] â†’ êµ­ë‚´ ì¤‘ì¥ë…„ì¸µ ê°€ìƒí™”í íˆ¬ì ì¦ê°€\n",
      "[ê°œì¸ì •ë³´ ìœ ì¶œÂ·ë³´í˜¸ì¡°ì¹˜_159] â†’ ì˜ˆìŠ¤24 ëœì„¬ì›¨ì–´ í•´í‚¹\n",
      "[ê¸ˆìœµì‚¬ê³ Â·ì†Œë¹„ì í”¼í•´_143] â†’ ì„œìš¸ ëŒ€í¬í†µì¥ ë³´ì´ìŠ¤í”¼ì‹± ì¡°ì§\n",
      "[ê¸ˆìœµì†Œë¹„ìë³´í˜¸ë²• ê¸°íƒ€_107] â†’ ë™êµ­ëŒ€ ì²­ë…„ ê¸ˆìœµêµìœ¡\n",
      "[ë²•ì  ë¶„ìŸÂ·ì •ì¹˜ ì—°ë£¨_164] â†’ ë¶€ì‚° ì „í•œê¸¸ ë‚´ë€ì„ ë™ ê³ ë°œ\n",
      "[ë²•ì  ë¶„ìŸÂ·ì •ì¹˜ ì—°ë£¨_171] â†’ ì„œìš¸ ê°•ë‚¨ ìœ íŠœë²„ í˜‘ë°• ì‚¬ê±´\n",
      "[ë²•ì  ë¶„ìŸÂ·ì •ì¹˜ ì—°ë£¨_172] â†’ ëŒ€êµ¬ ë¶ˆë²• ì—¬ë¡ ì¡°ì‚¬ ì‚¬ê±´\n",
      "[ë²•ì  ë¶„ìŸÂ·ì •ì¹˜ ì—°ë£¨_6] â†’ ì„œìš¸êµ¬ì¹˜ì†Œ ì„± ìƒë‚© ì˜í˜¹\n",
      "[ì‚°ì—…ì¬í•´ ì‚¬ê±´_194] â†’ íƒœì•ˆí™”ë ¥ í•˜ì²­ë…¸ë™ì ì‚¬ë§ì‚¬ê³ \n",
      "[ì‚°ì—…ì¬í•´ ì‚¬ê±´_197] â†’ ê²½ê¸°ë„ ì‹œí¥ ì œë¹µê³µì¥ ì‚¬ê³ \n",
      "[ì‚°ì—…ì¬í•´ ì‚¬ê±´_52] â†’ ìš¸ì‚° ì•ë°”ë‹¤ ì‘ì—…ì ì‚¬ë§ ì‚¬ê³ \n",
      "[ì‹œì¥Â·ê¸°ì—… ê´€ë ¨ ì‚¬ê±´_161] â†’ ì„œìš¸ SMì—”í„° ì‹œì„¸ì¡°ì¢… ì‚¬ê±´\n",
      "[ì•„ë™ ìœ ê¸°Â·ë°©ì„Â·ì‚¬ë§ ì‚¬ê±´_66] â†’ ì¸ì²œ ì´ˆë“±í•™ìƒ ë°©ì„ í™”ì¬\n",
      "[ì˜¨ë¼ì¸Â·í”Œë«í¼ ê´€ë ¨ ì´ìŠˆ_165] â†’ ì„œìš¸ ê°œì¸ì •ë³´ ë³´í˜¸ ì†Œì†¡\n",
      "[ì˜¨ë¼ì¸Â·í”Œë«í¼ ê´€ë ¨ ì´ìŠˆ_168] â†’ ì„¸ì¢…ì‹œ AI ë³´ì•ˆê´€ì œ ì¸ì¦\n",
      "[ì œë„Â·ì•ˆì „ê´€ë¦¬Â·ì •ì±…_153] â†’ ì¤‘ì†Œê¸°ì—… ì•ˆì „ë³´ê±´ ì§€ì›ì‚¬ì—…\n",
      "[ì œë„Â·ì•ˆì „ê´€ë¦¬Â·ì •ì±…_44] â†’ í•œêµ­ì˜ˆíƒê²°ì œì› ì¤‘ëŒ€ì¬í•´ ì¸ì¦\n",
      "[ì œë„Â·ì•ˆì „ê´€ë¦¬Â·ì •ì±…_52] â†’ ì„œìš¸ ê°•ë‚¨ ë…¸ë™ì •ì±… ì„¸ë¯¸ë‚˜\n",
      "[ì¤‘ëŒ€ì‹œë¯¼ì¬í•´_37] â†’ ì°½ì›NCíŒŒí¬ êµ¬ì¡°ë¬¼ ì¶”ë½ ì‚¬ê³ \n"
     ]
    }
   ],
   "source": [
    "# LLM ì´ë¦„ ë¶™ì´ê¸°\n",
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "df = pd.read_csv(\"../../data/processed/news_cluster.csv\")\n",
    "#df = df.sample(n=20, random_state=42) # í…ŒìŠ¤íŠ¸ìš©\n",
    "print(f\"ì´ {len(df)}ê±´ ë°ì´í„° ì‚¬ìš©\")\n",
    "\n",
    "# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¸íŒ…\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# GPT ìš”ì²­ í•¨ìˆ˜/í”„ë¡¬í”„íŠ¸\n",
    "def generate_event_name(representative: str, model: str = \"gpt-4o-mini\") -> str:\n",
    "    if pd.isna(representative) or not str(representative).strip():\n",
    "        return \"ëŒ€í‘œ ê¸°ì‚¬ ì—†ìŒ\"\n",
    "\n",
    "    system_message = (\n",
    "        \"ë‹¹ì‹ ì€ ëŒ€í‘œ ê¸°ì‚¬ë¥¼ ë³´ê³ , ì‚¬ê±´ì„ ëŒ€í‘œí•˜ëŠ” êµ¬ì²´ì ì¸ 'ì†Œë¶„ë¥˜' ì´ë¦„ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \"\n",
    "        \"ì†Œë¶„ë¥˜ëª…ì€ 5~20ì ì´ë‚´ì˜ ëª…ì‚¬í˜•ìœ¼ë¡œ, ì‚¬ê±´ì˜ ì£¼ìš” íŠ¹ì§•, 'ì¥ì†Œ', ëŒ€ìƒ, í–‰ìœ„ ë“±ì„ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤. \"\n",
    "        \"ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ í¬ê´„ì ì¸ ë‹¨ì–´ëŠ” í”¼í•˜ê³ , ì„¤ëª… ì—†ì´ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ ëª…ì‚¬êµ¬ë§Œ ì¶œë ¥í•˜ì„¸ìš”. \"\n",
    "        \"ì¥ì†Œê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”\"\n",
    "        \"ë°˜ë“œì‹œ ê° ì‚¬ê±´ëª…ì€ ê³ ìœ í•´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "    )\n",
    "\n",
    "    user_message = (\n",
    "        f\"ë‹¤ìŒ ëŒ€í‘œ ê¸°ì‚¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì´ í´ëŸ¬ìŠ¤í„°ì— ì í•©í•œ 'ì†Œë¶„ë¥˜' ì´ë¦„ì„ í•˜ë‚˜ ìƒì„±í•´ì£¼ì„¸ìš”.\\n\\n\"\n",
    "        f\"## ëŒ€í‘œ ê¸°ì‚¬:\\n{representative}\\n\\n\"\n",
    "        \"ì¡°ê±´:\\n\"\n",
    "        \"- 5~20ì, ëª…ì‚¬í˜• ë˜ëŠ” ì§§ì€ ëª…ì‚¬êµ¬\\n\"\n",
    "        \"- ì‚¬ê±´ì˜ ì£¼ìš” íŠ¹ì§•, ëŒ€ìƒ, ì¥ì†Œ, í–‰ìœ„ í¬í•¨\\n\"\n",
    "        \"- ì„¤ëª… ì—†ì´ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ ëª…ì‚¬êµ¬ë§Œ\\n\"\n",
    "        \"ì˜ˆì‹œ ì¶œë ¥:\\n\"\n",
    "        \"- ì„œìš¸ ê°•ë‚¨ ì•„íŒŒíŠ¸ í™”ì¬\\n\"\n",
    "        \"- ë¶€ì‚°ì‹œ ì´ˆë“±í•™ìƒ í•™ëŒ€ì‚¬ê±´\\n\"\n",
    "        \"- ì˜ë£Œê¸°ê´€ ê°œì¸ì •ë³´ ìœ ì¶œ\\n\"\n",
    "        \"- ì´í•˜ëŠ˜ ì‚¬ì´ë²„ ëª…ì˜ˆí›¼ì† ê²Œì‹œê¸€\\n\"\n",
    "        \"ì´ í˜•ì‹ì— ë§ê²Œ ìƒì„±í•´ì£¼ì„¸ìš”:\"\n",
    "    )\n",
    "\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": user_message},\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=30,\n",
    "                n=1,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                continue\n",
    "            return f\"[ìƒì„± ì‹¤íŒ¨: {e}]\"\n",
    "\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ê¸°ì‚¬ë¡œ ì†Œë¶„ë¥˜ëª… ìƒì„±\n",
    "cluster_names = {}\n",
    "print(\"\\n=== í´ëŸ¬ìŠ¤í„° ì†Œë¶„ë¥˜ëª… ìƒì„± ê²°ê³¼ ===\")\n",
    "for cluster_name, sub in df.groupby(\"cluster_name\"):\n",
    "    representative = sub[\"representative\"].iloc[0]  # ëŒ€í‘œ ê¸°ì‚¬ 1ê°œ ì´ìš©\n",
    "    event_name = generate_event_name(representative)\n",
    "    print(f\"[{cluster_name}] â†’ {event_name}\")\n",
    "    cluster_names[cluster_name] = event_name\n",
    "\n",
    "df[\"cluster_event_name\"] = df[\"cluster_name\"].map(cluster_names)\n",
    "df.to_csv(\"../../data/processed/news_cluster_w_event_name.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df1c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… '../../data/processed/news_cluster_w_event_name.csv' íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. (ì´ 20ê°œ í–‰)\n",
      "\n",
      "â–¶ ë‚ ì§œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ 'ì—°ë„-ì›”' ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\n",
      "â–¶ ë‚ ì§œ ì²˜ë¦¬ ì™„ë£Œ.\n",
      "\n",
      "â–¶ ì›”ë³„, 'MAIN_NAME'ë³„ ìµœë‹¤ ë°œìƒ í´ëŸ¬ìŠ¤í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì›”ë³„/ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 728.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š ì›”ë³„ / MAIN_NAMEë³„ ìµœë‹¤ ë°œìƒ í´ëŸ¬ìŠ¤í„° ì´ë²¤íŠ¸ ìš”ì•½\n",
      " Month      MAIN_NAME  Top Cluster Event  Count  Total Articles  Percentage (%)\n",
      "202508 ì•„ë™ ìœ ê¸°Â·ë°©ì„Â·ì‚¬ë§ ì‚¬ê±´      ì¸ì²œ ì´ˆë“±í•™ìƒ ë°©ì„ í™”ì¬      1               1           100.0\n",
      "202508     ì œë„Â·ì•ˆì „ê´€ë¦¬Â·ì •ì±…    í•œêµ­ì˜ˆíƒê²°ì œì› ì¤‘ëŒ€ì¬í•´ ì¸ì¦      1               1           100.0\n",
      "202507    ì‹œì¥Â·ê¸°ì—… ê´€ë ¨ ì‚¬ê±´    ì„œìš¸ SMì—”í„° ì‹œì„¸ì¡°ì¢… ì‚¬ê±´      1               1           100.0\n",
      "202507     ì œë„Â·ì•ˆì „ê´€ë¦¬Â·ì •ì±…     ì„œìš¸ ê°•ë‚¨ ë…¸ë™ì •ì±… ì„¸ë¯¸ë‚˜      1               1           100.0\n",
      "202506   ê°œì¸ì •ë³´ ìœ ì¶œÂ·ë³´í˜¸ì¡°ì¹˜       ì˜ˆìŠ¤24 ëœì„¬ì›¨ì–´ í•´í‚¹      1               1           100.0\n",
      "202506    ë²•ì  ë¶„ìŸÂ·ì •ì¹˜ ì—°ë£¨      ì„œìš¸êµ¬ì¹˜ì†Œ ì„± ìƒë‚© ì˜í˜¹      1               1           100.0\n",
      "202506        ì‚°ì—…ì¬í•´ ì‚¬ê±´     ê²½ê¸°ë„ ì‹œí¥ ì œë¹µê³µì¥ ì‚¬ê³       1               1           100.0\n",
      "202505    ê¸ˆìœµì†Œë¹„ìë³´í˜¸ë²• ê¸°íƒ€        ë™êµ­ëŒ€ ì²­ë…„ ê¸ˆìœµêµìœ¡      1               1           100.0\n",
      "202505        ì‚°ì—…ì¬í•´ ì‚¬ê±´   ìš¸ì‚° ì•ë°”ë‹¤ ì‘ì—…ì ì‚¬ë§ ì‚¬ê³       1               1           100.0\n",
      "202505  ì˜¨ë¼ì¸Â·í”Œë«í¼ ê´€ë ¨ ì´ìŠˆ      ì„œìš¸ ê°œì¸ì •ë³´ ë³´í˜¸ ì†Œì†¡      1               1           100.0\n",
      "202504    ê¸ˆìœµì‚¬ê³ Â·ì†Œë¹„ì í”¼í•´   ì„œìš¸ ëŒ€í¬í†µì¥ ë³´ì´ìŠ¤í”¼ì‹± ì¡°ì§      1               1           100.0\n",
      "202504    ë²•ì  ë¶„ìŸÂ·ì •ì¹˜ ì—°ë£¨      ëŒ€êµ¬ ë¶ˆë²• ì—¬ë¡ ì¡°ì‚¬ ì‚¬ê±´      1               2            50.0\n",
      "202504         ì¤‘ëŒ€ì‹œë¯¼ì¬í•´   ì°½ì›NCíŒŒí¬ êµ¬ì¡°ë¬¼ ì¶”ë½ ì‚¬ê³       1               1           100.0\n",
      "202503      ê°€ìƒìì‚°Â·ê·œì œì •ì±… êµ­ë‚´ ì¤‘ì¥ë…„ì¸µ ê°€ìƒí™”í íˆ¬ì ì¦ê°€      1               1           100.0\n",
      "202502      ê°€ìƒìì‚°Â·ê·œì œì •ì±…       êµ­íšŒ ìƒì†ì„¸ ê°œì • ë…¼ì˜      1               1           100.0\n",
      "202502    ë²•ì  ë¶„ìŸÂ·ì •ì¹˜ ì—°ë£¨     ë¶€ì‚° ì „í•œê¸¸ ë‚´ë€ì„ ë™ ê³ ë°œ      1               1           100.0\n",
      "202502        ì‚°ì—…ì¬í•´ ì‚¬ê±´    íƒœì•ˆí™”ë ¥ í•˜ì²­ë…¸ë™ì ì‚¬ë§ì‚¬ê³       1               1           100.0\n",
      "202502     ì œë„Â·ì•ˆì „ê´€ë¦¬Â·ì •ì±…     ì¤‘ì†Œê¸°ì—… ì•ˆì „ë³´ê±´ ì§€ì›ì‚¬ì—…      1               1           100.0\n",
      "202501  ì˜¨ë¼ì¸Â·í”Œë«í¼ ê´€ë ¨ ì´ìŠˆ     ì„¸ì¢…ì‹œ AI ë³´ì•ˆê´€ì œ ì¸ì¦      1               1           100.0\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# LLM ì´ë¦„ë¶™ì¸ ê²°ê³¼ í™•ì¸ìš© -> ì›”ë³„ ìµœë‹¤ ì‚¬ê±´ëª… count\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "FILE_PATH =\"../../data/processed/news_cluster_w_event_name.csv\"\n",
    "DATE_COL = \"date\"\n",
    "MAIN_NAME_COL = \"MAIN_NAME\" \n",
    "CLUSTER_NAME_COL = \"cluster_event_name\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(FILE_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ì˜¤ë¥˜: '{FILE_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "    exit()\n",
    "\n",
    "# í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "required_cols = [DATE_COL, MAIN_NAME_COL, CLUSTER_NAME_COL]\n",
    "if not all(col in df.columns for col in required_cols):\n",
    "    print(f\"ì˜¤ë¥˜: íŒŒì¼ì— '{', '.join(required_cols)}' ì»¬ëŸ¼ì´ ëª¨ë‘ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "    print(f\"    í˜„ì¬ íŒŒì¼ì— ìˆëŠ” ì»¬ëŸ¼: {df.columns.tolist()}\")\n",
    "    exit()\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬: 'year_month' ì»¬ëŸ¼ ìƒì„±\n",
    "print(\"â–¶ ë‚ ì§œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ 'ì—°ë„-ì›”' ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\")\n",
    "# date ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬ (ì˜¤ë¥˜ ë°©ì§€)\n",
    "df[DATE_COL] = df[DATE_COL].astype(str)\n",
    "# ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ ë°ì´í„°ë§Œ í•„í„°ë§ (ì˜ˆ: 14ìë¦¬)\n",
    "df = df[df[DATE_COL].str.len() >= 6].copy()\n",
    "# ì•ì—ì„œ 6ìë¦¬ (YYYYMM)ë¥¼ ì˜ë¼ 'year_month' ì»¬ëŸ¼ ìƒì„±\n",
    "df['year_month'] = df[DATE_COL].str[:6]\n",
    "print(\"â–¶ ë‚ ì§œ ì²˜ë¦¬ ì™„ë£Œ.\\n\")\n",
    "\n",
    "\n",
    "# ì›”ë³„, MAIN_NAMEë³„ ìµœë‹¤ í´ëŸ¬ìŠ¤í„° ë¶„ì„\n",
    "print(f\"â–¶ ì›”ë³„, '{MAIN_NAME_COL}'ë³„ ìµœë‹¤ ë°œìƒ í´ëŸ¬ìŠ¤í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤...\")\n",
    "# ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "monthly_summary = []\n",
    "# 'year_month'ì™€ 'MAIN_NAME'ìœ¼ë¡œ ê·¸ë£¹í™”\n",
    "grouped = df.groupby(['year_month', MAIN_NAME_COL])\n",
    "for (year_month, main_name), group in tqdm(grouped, desc=\"ì›”ë³„/ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ì¤‘\"):\n",
    "    # ê° ê·¸ë£¹ ë‚´ì—ì„œ cluster_event_nameì˜ ê°œìˆ˜ë¥¼ ì…ˆ\n",
    "    cluster_counts = group[CLUSTER_NAME_COL].value_counts()\n",
    "    # ê·¸ë£¹ì— ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°\n",
    "    if cluster_counts.empty:\n",
    "        continue\n",
    "    # ê°€ì¥ ë§ì´ ë°œìƒí•œ í´ëŸ¬ìŠ¤í„°ì˜ ì´ë¦„ê³¼ íšŸìˆ˜ ì¶”ì¶œ\n",
    "    top_cluster_name = cluster_counts.index[0]\n",
    "    top_cluster_count = cluster_counts.iloc[0]\n",
    "    # í•´ë‹¹ ê·¸ë£¹ì˜ ì „ì²´ ê¸°ì‚¬ ìˆ˜\n",
    "    total_articles_in_group = len(group)\n",
    "    \n",
    "    monthly_summary.append({\n",
    "        'Month': year_month,\n",
    "        'MAIN_NAME': main_name,\n",
    "        'Top Cluster Event': top_cluster_name,\n",
    "        'Count': top_cluster_count,\n",
    "        'Total Articles': total_articles_in_group\n",
    "    })\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "if not monthly_summary:\n",
    "    print(\"\\në¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    summary_df = pd.DataFrame(monthly_summary)\n",
    "    summary_df['Percentage (%)'] = (summary_df['Count'] / summary_df['Total Articles'] * 100).round(2)\n",
    "    summary_df = summary_df.sort_values(by=['Month', 'MAIN_NAME'], ascending=[False, True])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ğŸ“Š ì›”ë³„ / MAIN_NAMEë³„ ìµœë‹¤ ë°œìƒ í´ëŸ¬ìŠ¤í„° ì´ë²¤íŠ¸ ìš”ì•½\")\n",
    "\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
